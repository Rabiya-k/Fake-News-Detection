# -*- coding: utf-8 -*-
"""NLP: FakeNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zfk355B9hJU7cSC9hzklSoqQozVZOCiz

**Import Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import pickle

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer

from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer

"""**Read the dataset**"""

data = pd.read_csv('venv/news.csv')

data.head()

data.shape

data.info()

data.isnull().sum()

data['label'] = data['label'].replace({'REAL': 1, 'FAKE': 0})
data['label'].value_counts()

"""Let's combine the title and text together"""

data['text'] = data.apply(lambda row: row['title'] + ' ' + row['text'], axis=1)
del data['title']
#data.head()

data.iloc[1]['text']

"""**Visualization**

1.Count of Fake and Real Data
"""

print(data["label"].value_counts())
fig, ax = plt.subplots(1,2, figsize=(12, 6))
bar = data['label'].value_counts().plot.bar(ax = ax[0])
bar.set_title("Count of real and fake data")
bar.set_ylabel("Count")
bar.set_xlabel("Label")
pie = plt.pie(data["label"].value_counts().values,explode=[0,0],labels=data.label.value_counts().index, autopct='%1.1f%%',colors=['SkyBlue','PeachPuff'])
fig.show()

"""2.create a wordcloud for fake and real news"""

# separate texts with label = 1
plt.figure(figsize = (10,10))
text_1 = ' '.join(data[data['label']==1]['text'])

# create a word cloud
wordcloud_1 = WordCloud().generate(text_1)

plt.imshow(wordcloud_1, interpolation='bilinear')
plt.axis("off")
plt.show()

# repeat for label = 0
plt.figure(figsize = (10,10))
text_0 = ' '.join(data[data['label']==0]['text'])
wordcloud_0 = WordCloud().generate(text_0)
plt.imshow(wordcloud_0, interpolation='bilinear')
plt.axis("off")
plt.show()

"""**Removing punctuations**

**Funtion to remove punctuation**
"""

def remove_punctuation(text):
    '''a function for removing punctuation'''
    import string
    # replacing the punctuations with no space,
    # which in effect deletes the punctuation marks
    translator = str.maketrans('', '', string.punctuation)
    # return the text stripped of punctuation marks
    return text.translate(translator)

#Apply the function to each examples
data['text'] = data['text'].apply(remove_punctuation)
data.head(10)

data.iloc[1]['text']

"""**Removing stopwords**

### Extract the stop words
"""

import nltk
nltk.download('stopwords')

# extracting the stopwords from nltk library
sw = stopwords.words('english')
# displaying the stopwords
np.array(sw)

print("Number of stopwords: ", len(sw))

"""**Function to remove stopwords**"""

def stopwords(text):
    '''a function for removing the stopword'''
    # removing the stop words and lowercasing the selected words
    text = [word.lower() for word in text.split() if word.lower() not in sw]
    # joining the list of words with space separator
    return " ".join(text)

data['text'] = data['text'].apply(stopwords)
data.head(10)

data.iloc[1]['text']

"""Now let’s create a wordcloud for the cleaned data"""

from wordcloud import WordCloud,STOPWORDS
plt.figure(figsize = (10,10))
wc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(" ".join(data[data.label == 1].text))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (10,10))
wc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(" ".join(data[data.label == 0].text))
plt.imshow(wc , interpolation = 'bilinear')

"""Number of words in each text"""

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,6))
text_len=data[data['label']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(text_len,color='SkyBlue')
ax1.set_title('Real news text')
text_len=data[data['label']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(text_len,color='PeachPuff')
ax2.set_title('Fake news text')
fig.suptitle('Words in texts')
plt.show()

"""The number of words seems to be a bit different. 1000 words are most common in real news category while around 500 words are most common in fake news category.

---


"""

X = data['text']
y = data['label']


#split train and test data
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=0)

"""**Vectorization**"""

vectorizer =TfidfVectorizer()
tfidf_X_train = vectorizer.fit_transform(X_train)
tfidf_X_test = vectorizer.transform(X_test)

"""**Modeling**

PASSIVE-AGGRESSIVE CLASSIFIER
"""

from sklearn.metrics import accuracy_score
from sklearn.linear_model import PassiveAggressiveClassifier
classifier=PassiveAggressiveClassifier(max_iter=50)
classifier.fit(tfidf_X_train,y_train)

pickle.dump(classifier, open('model.pkl', 'wb'))
pickle.dump(vectorizer, open('tfidfvect2.pkl', 'wb'))


#Predict on the test set and calculate accuracy
y_pred= classifier.predict(tfidf_X_test)
score=accuracy_score(y_test,y_pred)
print(f'Accuracy: {round(score*100,2)}%')

print(classification_report(y_test, y_pred, target_names = ['Fake','Real']))

y

y_pred

"""**Prediction**"""

#def news_detector(news):
    #input_data=[news]
    #vectorization
    #vectorized_input_data=vectorizer.transform(input_data)
    #predicton
    #prediction = classifier.predict(vectorized_input_data)
    # print(prediction)
    #if(prediction[0]==0):
        #print("Looks like a FAKE News")
    #else:
        #print("Looks like a REAL News")

"""**Testing on random examples**"""

#news_detector("Go to Article President Barack Obama has been campaigning hard for the woman who is supposedly going to extend his legacy")

#news_detector("After a week of nonstop criticism from Democrats and Republicans alike for comments many condemned as racially charged, Donald Trump claims to be altering his campaign to be a little more inclusive. While the presumptive G.O.P. has long promised to â€œmake America great again,â€ Trump now says heâ€™s adding two words to slogan to illustrate just how non-racist he really is.â€œYou know, I have the theme â€˜make America great again,â€™ and I've added a couple of things,â€ Trump announced to supporters at a campaign rally in Richmond, Virginia, on Friday night. â€œRight now Iâ€™m adding make America great againâ€”Iâ€™m adding â€˜for everyone,â€™ because itâ€™s really going to be for everyone. Itâ€™s not going to be for a group of people, itâ€™s going to be for everyone. Itâ€™s true.â€The allegedly amended slogan, which has yet to appear on any official signage or Trump merchandise, comes after the presidential candidate spent the first half of June repeatedly denouncing Gonzalo Curiel, the federal judge of Mexican heritage presiding over the Trump University class action lawsuit, as inherently biased against him. (Curiel was born in Indiana.) His comments were widely condemned by the Washington political establishment, including Senate Minority Leader Mitch McConnell, who suggested he may be an idiot, and House Speaker Paul Ryan, who called Trumpâ€™s statement the â€œtextbook definition of a racist comment.â€Trump, who hasnâ€™t apologized or taken back any of his comments, indicated on Friday that he realized his words have had a negative effect on his campaign and declared he is not a racist.â€œI am the least racist person. The least racist person that youâ€™ve ever seen. I mean give me a break,â€ he said at the rally. â€œI am the least racist person that youâ€™ve ever looked at, believe me.â€")

#news_detector(data.iloc[0]['text'])

#news_detector(data.iloc[1]['text'])

#news_detector(data.iloc[2]['text'])

#news_detector(data.iloc[3]['text'])

#news_detector(data.iloc[20]['text'])

